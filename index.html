<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="SurgCUT3R: Surgical Scene-Aware Continuous Understanding of Temporal 3D Representation">
  <meta name="description" content="SurgCUT3R adapts unified online 3D reconstruction to monocular surgical endoscopy via metric-scale pseudo-GT depth, hybrid supervision, and hierarchical inference to reduce long-sequence drift.">
  <meta name="keywords" content="surgical reconstruction, monocular endoscopy, continuous 3D reconstruction, pose drift, pseudo ground truth, hybrid supervision, CUT3R, DUSt3R, SCARED, StereoMIS">
  <meta name="author" content="Kaiyuan Xu, Fangzhou Hong, Daniel Elson, Baoru Huang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="SurgCUT3R">
  <meta property="og:title" content="SurgCUT3R: Surgical Scene-Aware Continuous Understanding of Temporal 3D Representation">
  <meta property="og:description" content="A unified framework for monocular surgical 3D reconstruction with metric-scale pseudo-GT depth, hybrid supervision, and hierarchical inference for long-sequence stability.">
  <!-- TODO: Replace with the final project page URL once deployed -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/SurgCUT3R">
  <!-- TODO: Add a 1200x630px image at static/images/social_preview.png -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="SurgCUT3R - Research Preview">
  <meta property="article:published_time" content="2026-02-24T00:00:00.000Z">
  <meta property="article:author" content="Kaiyuan Xu">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Surgical Reconstruction">
  <meta property="article:tag" content="Monocular 3D">



  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="SurgCUT3R: Surgical Scene-Aware Continuous Understanding of Temporal 3D Representation">
  <meta name="citation_author" content="Xu, Kaiyuan">
  <meta name="citation_author" content="Hong, Fangzhou">
  <meta name="citation_author" content="Elson, Daniel">
  <meta name="citation_author" content="Huang, Baoru">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_conference_title" content="IEEE International Conference on Robotics and Automation (ICRA)">
  <!-- TODO: Replace with the absolute URL to your hosted PDF for best indexing -->
  <meta name="citation_pdf_url" content="static/pdfs/_2026__ICRA__Kaiyuan_Xu__4D_reconstruction_%20(1).pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>


  <title>SurgCUT3R | ICRA 2026</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 64 64'%3E%3Crect width='64' height='64' rx='14' fill='%232563eb'/%3E%3Ctext x='32' y='40' text-anchor='middle' font-size='26' font-family='Inter,Arial' fill='white'%3ESC%3C/text%3E%3C/svg%3E">
  <link rel="apple-touch-icon" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 64 64'%3E%3Crect width='64' height='64' rx='14' fill='%232563eb'/%3E%3Ctext x='32' y='40' text-anchor='middle' font-size='26' font-family='Inter,Arial' fill='white'%3ESC%3C/text%3E%3C/svg%3E">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "SurgCUT3R: Surgical Scene-Aware Continuous Understanding of Temporal 3D Representation",
    "description": "A unified framework for monocular surgical 3D reconstruction with metric-scale pseudo-GT depth, hybrid supervision, and hierarchical inference for long-sequence stability.",
    "author": [
      {
        "@type": "Person",
        "name": "Kaiyuan Xu"
      },
      {
        "@type": "Person",
        "name": "Fangzhou Hong"
      },
      {
        "@type": "Person",
        "name": "Daniel Elson"
      },
      {
        "@type": "Person",
        "name": "Baoru Huang"
      }
    ],
    "datePublished": "2026-02-24",
    "publisher": {
      "@type": "Organization",
      "name": "IEEE International Conference on Robotics and Automation (ICRA)"
    },
    "url": "https://YOUR_DOMAIN.com/SurgCUT3R",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["surgical reconstruction", "monocular endoscopy", "continuous 3D reconstruction", "pose drift", "pseudo ground truth", "hybrid supervision", "CUT3R"],
    "abstract": "Reconstructing surgical scenes from monocular endoscopic video is critical for advancing robotic-assisted surgery. However, the application of state-of-the-art general-purpose reconstruction models is constrained by two key challenges: the lack of supervised training data and performance degradation over long video sequences. To overcome these limitations, we propose SurgCUT3R, a systematic framework that adapts unified 3D reconstruction models to the surgical domain. Our contributions are threefold. First, we develop a data generation pipeline that exploits public stereo surgical datasets to produce large-scale, metric-scale pseudo-ground-truth depth maps, effectively bridging the data gap. Second, we propose a hybrid supervision strategy that couples our pseudo-ground-truth with geometric self-correction to enhance robustness against inherent data imperfections. Third, we introduce a hierarchical inference framework that employs two specialized models to effectively mitigate accumulated pose drift over long surgical videos: one for global stability and one for local accuracy. Experiments on the SCARED and StereoMIS datasets demonstrate that our method achieves a competitive balance between accuracy and efficiency, delivering near state-of-the-art but substantially faster pose estimation and offering a practical and effective solution for robust reconstruction in surgical environments.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/SurgCUT3R"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Surgical scene reconstruction"
      },
      {
        "@type": "Thing", 
        "name": "Online monocular 3D reconstruction"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "SurgCUT3R Project",
    "url": "https://YOUR_DOMAIN.com/SurgCUT3R",
    "logo": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "sameAs": [
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- Related works dropdown removed (no links provided). -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SurgCUT3R: Surgical Scene-Aware Continuous Understanding of Temporal 3D Representation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Kaiyuan Xu<sup>1</sup>,</span>
              <span class="author-block">
                Fangzhou Hong<sup>2</sup>,</span>
              <span class="author-block">
                Daniel Elson<sup>1</sup>,</span>
              <span class="author-block">
                Baoru Huang<sup>1,3</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>The Hamlyn Centre for Robotic Surgery, Imperial College London, UK<br>
                      <sup>2</sup>S-Lab, College of Computing and Data Science, Nanyang Technological University, Singapore<br>
                      <sup>3</sup>Department of Computer Science, University of Liverpool, UK<br>
                      ICRA 2026
                    </span>
                  </div>

                  <div class="publication-links">
                    <span class="link-block">
                      <a href="static/pdfs/_2026__ICRA__Kaiyuan_Xu__4D_reconstruction_%20(1).pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="javascript:void(0)" aria-disabled="true"
                    class="external-link button is-normal is-rounded is-dark is-disabled">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
              </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Quick Summary -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <p>
            SurgCUT3R adapts a state-of-the-art unified online reconstruction model to monocular surgical endoscopic video, addressing
            <strong>(i)</strong> the lack of supervised training data and <strong>(ii)</strong> accumulated pose drift on long sequences.
          </p>
          <ul>
            <li><strong>Metric-scale pseudo-GT depth</strong> from public stereo surgical datasets (SCARED, StereoMIS).</li>
            <li><strong>Hybrid supervision</strong> combining pseudo-GT with geometric self-correction to resist label noise.</li>
            <li><strong>Hierarchical long-sequence inference</strong> using global stability + local accuracy models to suppress drift.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reconstructing surgical scenes from monocular endoscopic video is critical for advancing robotic-assisted surgery. However, the application of state-of-the-art general-purpose reconstruction models is constrained by two key challenges: the lack of supervised training data and performance degradation over long video sequences. To overcome these limitations, we propose SurgCUT3R, a systematic framework that adapts unified 3D reconstruction models to the surgical domain. Our contributions are threefold. First, we develop a data generation pipeline that exploits public stereo surgical datasets to produce large-scale, metric-scale pseudo-ground-truth depth maps, effectively bridging the data gap. Second, we propose a hybrid supervision strategy that couples our pseudo-ground-truth with geometric self-correction to enhance robustness against inherent data imperfections. Third, we introduce a hierarchical inference framework that employs two specialized models to effectively mitigate accumulated pose drift over long surgical videos: one for global stability and one for local accuracy. Experiments on the SCARED and StereoMIS datasets demonstrate that our method achieves a competitive balance between accuracy and efficiency, delivering near state-of-the-art but substantially faster pose estimation and offering a practical and effective solution for robust reconstruction in surgical environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <figure class="paper-figure">
          <img src="static/images/pipeline.png" alt="Overview of SurgCUT3R pipeline and hierarchical inference" loading="lazy" decoding="async">
          <figcaption>
            <strong>Overview of SurgCUT3R.</strong>
            <strong>Left:</strong> The unified reconstruction pipeline. Streaming video frames are encoded via a ViT encoder and interact with a persistent state, which is continuously updated to sequentially output the pointmap and camera parameter for each frame.
            <strong>Right:</strong> Our hierarchical framework for long-sequence inference. The pink lines represent camera trajectories. A sparse but globally stable trajectory from a global model (<em>M</em><sub>global</sub>) provides anchor points to correct and stitch the dense but locally drifting trajectories from a local model (<em>M</em><sub>local</sub>), producing a final, drift-corrected trajectory.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <figure class="paper-figure">
          <img src="static/images/quantitative.png" alt="Quantitative evaluation table" loading="lazy" decoding="async">
          <figcaption>
            <strong>Quantitative results.</strong> Quantitative evaluation of our SurgCUT3R method against existing methods in endoscopic scene reconstruction. The optimal and suboptimal results are shown in bold and underlined respectively.
          </figcaption>
        </figure>

        <figure class="paper-figure">
          <img src="static/images/3Dvisualization.png" alt="Qualitative results of 3D reconstruction" loading="lazy" decoding="async">
          <figcaption>
            <strong>Qualitative results of 3D reconstruction.</strong> With videos (small images) as input, this figure shows the reconstruction from the first frame (large images left) and the accumulated 3D model from multiple frames (large images right). This alignment between the single-frame and multi-frame reconstruction results highlights the geometric consistency of our method.
          </figcaption>
        </figure>

        <figure class="paper-figure">
          <img src="static/images/qualitative.png" alt="Qualitative results of monocular depth estimation" loading="lazy" decoding="async">
          <figcaption>
            <strong>Qualitative results of monocular depth estimation.</strong> We compare our method with MonST3R [24], Spann3R [25], AF-SfMLearner [29], EndoDAC [30] and MegaSaM [28] on SCARED [34] and StereoMIS [35] datasets. Our method achieves the best qualitative results in feed-forward methods.
          </figcaption>
        </figure>

        <figure class="paper-figure">
          <img src="static/images/ablation2.jpg" alt="Qualitative comparison of camera trajectories (ablation)" loading="lazy" decoding="async">
          <figcaption>
            <strong>Qualitative comparison of camera trajectories.</strong> Left: Without the hierarchical inference framework. Right: With our hierarchical inference framework (Ours).
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{xu2026surgcut3r,
  title     = {SurgCUT3R: Surgical Scene-Aware Continuous Understanding of Temporal 3D Representation},
  author    = {Xu, Kaiyuan and Hong, Fangzhou and Elson, Daniel and Huang, Baoru},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  year      = {2026},
  url       = {https://YOUR_DOMAIN.com/SurgCUT3R}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  </main>

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
